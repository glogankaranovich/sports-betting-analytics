# Sports Betting Analytics - Development Workflow

## ðŸŽ¯ Daily Development Process

### AWS Profile Configuration
- **Dev Account Profile**: `sports-betting-dev`
- All dev environment operations use this profile automatically via Makefile
- For manual AWS CLI commands, always specify: `AWS_PROFILE=sports-betting-dev`

### Step-by-Step Workflow
1. **Identify Task**: Review PROJECT_STATUS.md and select next task
2. **Update Status**: Mark task as "In Progress" in PROJECT_STATUS.md
3. **Implement**: Write code for the task
4. **Test**: Create unit tests and integration tests as needed
5. **Build Check**: Run `make workflow-check` to verify everything works
6. **Fix Issues**: Address any linting, test, or build failures
7. **Deploy Dev** (if CDK changes): Deploy to dev stack and verify
8. **Update Docs**: Mark task complete in PROJECT_STATUS.md
9. **Log Learnings**: Add insights to TROUBLESHOOTING_AND_LESSONS.md
10. **Commit & Push**: Commit changes and push to trigger pipeline
11. **Monitor Pipeline**: Check deployment status asynchronously

### Quality Gates
- **All tests pass** before committing
- **High test coverage** maintained
- **Code automatically linted** and fixed
- **Build succeeds** on all components
- **Dev deployment works** (for infrastructure changes)

## ðŸ“ Task Management

### PROJECT_STATUS.md Format
Tasks should be tracked with these statuses:
- **â³ Not Started** - Task identified but not begun
- **ðŸš§ In Progress** - Currently working on this task
- **âœ… Complete** - Task finished and tested
- **ðŸ”„ Testing** - Implementation done, testing in progress
- **ðŸš€ Deploying** - Deployed to dev, verifying functionality

### Status Update Commands
```bash
# Quick status check
make status-check

# Update task status (interactive)
make update-task-status
```

## ðŸ§ª Testing & Quality Assurance

### Automated Quality Checks
```bash
# Run full workflow check (linting, tests, build)
make workflow-check

# Individual components
make lint-fix      # Auto-fix linting issues
make test         # Run all tests
make build        # Build all components
```

### Test Coverage Requirements
- **Unit tests** for all new code
- **Integration tests** for API endpoints and data flows
- **High coverage** maintained (aim for >90% line and branch coverage)
- **All tests pass** before any commit
- **Coverage commands**:
  - `make test-coverage` - Full backend coverage report
  - `make test-coverage-module MODULE=<module_name>` - Specific module coverage
  - Both commands show line and branch coverage metrics

### Writing Unit Tests - Critical Rules
**ALWAYS read the implementation code FIRST before writing tests:**

1. **Read the actual implementation** to understand:
   - Method signatures and parameters
   - Dependencies (e.g., `self.table` from BaseAPIHandler)
   - Required vs optional parameters
   - Expected return values and data structures
   - Error handling behavior

2. **Check existing test patterns** in the codebase:
   - How dependencies are mocked
   - Import patterns and environment setup
   - Assertion styles used

3. **Write tests that match reality**:
   - Use actual method signatures from the code
   - Mock all dependencies the code actually uses
   - Test with realistic data structures
   - Verify actual return values, not assumptions

4. **Common pitfalls to avoid**:
   - âŒ Guessing method signatures without reading code
   - âŒ Mocking wrong import paths
   - âŒ Testing with incomplete mock data
   - âŒ Asserting on wrong response keys
   - âœ… Read implementation â†’ Understand dependencies â†’ Write matching tests

**Example workflow:**
```bash
# 1. Read the implementation
cat backend/api/games.py | grep "def get_games"

# 2. Check what it imports/uses
cat backend/api/games.py | grep "import\|from"

# 3. Look at similar existing tests
cat backend/tests/unit/test_api_handler.py

# 4. Write test matching actual implementation
```

### Code Quality Standards
- **Automatic linting** with auto-fix enabled
- **Python**: black + flake8 + isort
- **TypeScript**: ESLint + Prettier
- **Consistent formatting** across all files

## ðŸš€ Deployment & Pipeline Monitoring

### Development Environment
```bash
# Deploy to dev (manual only) - uses sports-betting-dev profile
make deploy-dev

# Verify dev deployment
make verify-dev

# Clear DynamoDB table (use ops directory)
cd ../ops && AWS_PROFILE=sports-betting-dev python3 clear_table.py

### Lambda Testing Workflow
Complete testing sequence for Lambda functions:

```bash
# 1. Clear DynamoDB table
cd ../ops && AWS_PROFILE=sports-betting-dev python3 clear_table.py

# 2. Deploy all stacks to dev
cd ../infrastructure && make deploy-dev

# 3. Test Lambda functions in order:

# Step 3a: Test Odds Collector
AWS_PROFILE=sports-betting-dev aws lambda invoke --function-name Dev-OddsCollector-OddsCollectorFunction6A9C6277-RXibwu37zOli /tmp/response.json
cat /tmp/response.json
# Verify: Check DynamoDB for game/prop data

# Step 3b: Test Prediction Generator  
echo '{"sport":"basketball_nba","model":"consensus","bet_type":"props","limit":5}' | base64 | tr -d '\n' > /tmp/payload.b64
AWS_PROFILE=sports-betting-dev aws lambda invoke --function-name Dev-PredictionGenerator-PredictionGeneratorFunctio-nUbb1Nq1bDK4 --payload file:///tmp/payload.b64 /tmp/response.json
cat /tmp/response.json
# Verify: Check DynamoDB for prediction data

# Step 3c: Test Recommendation Generator
echo '{"sport":"basketball_nba","bookmaker":"fanduel","model":"consensus","risk_level":"conservative","bet_type":"props"}' | base64 | tr -d '\n' > /tmp/payload.b64
AWS_PROFILE=sports-betting-dev aws lambda invoke --function-name Dev-RecommendationGenerat-RecommendationGeneratorF-94DTeKirnv2I --payload file:///tmp/payload.b64 /tmp/response.json
cat /tmp/response.json
# Verify: Check response for recommendations

# 4. Check logs for any errors (replace FUNCTION_NAME and STREAM_NAME)
AWS_PROFILE=sports-betting-dev aws logs describe-log-streams --log-group-name /aws/lambda/FUNCTION_NAME --order-by LastEventTime --descending --max-items 1 --query 'logStreams[0].logStreamName' --output text
AWS_PROFILE=sports-betting-dev aws logs get-log-events --log-group-name /aws/lambda/FUNCTION_NAME --log-stream-name STREAM_NAME

# 5. If errors found: Fix code, redeploy, repeat testing
```

**Testing Rules:**
- Always test in order: Odds Collector â†’ Prediction Generator â†’ Recommendation Generator
- Verify DynamoDB state and Lambda logs after each step
- Fix any errors immediately before proceeding to next Lambda
- Clear table and redeploy if major changes are made

# Check dev resources
AWS_PROFILE=sports-betting-dev aws cloudformation list-stacks --stack-status-filter CREATE_COMPLETE UPDATE_COMPLETE --query 'StackSummaries[].StackName' --output table

# View Lambda logs
AWS_PROFILE=sports-betting-dev aws logs describe-log-streams --log-group-name /aws/lambda/FUNCTION_NAME --order-by LastEventTime --descending --max-items 1 --query 'logStreams[0].logStreamName' --output text
AWS_PROFILE=sports-betting-dev aws logs get-log-events --log-group-name /aws/lambda/FUNCTION_NAME --log-stream-name STREAM_NAME
```

### Pipeline Monitoring
```bash
# Check pipeline status
make check-pipeline

# Get pipeline logs (if failed)
make pipeline-logs

# Monitor deployment progress
make monitor-deployment
```

### Environment Strategy
- **Dev**: Manual deployment for testing with real AWS resources
- **Staging/Prod**: Automatic deployment through pipeline only
- **Pipeline**: Triggers on push to main branch

## ðŸ”„ Example Development Session

1. **Start**: Review current status and recent lessons
2. **Plan**: "Add NBA API integration for game data"
3. **Test**: Write tests for API client and data parsing
4. **Implement**: FastAPI endpoints and data models
5. **Verify**: Deploy to Dev, test API endpoints
6. **Document**: Update status with completed work
7. **Capture**: Log decision rationale and any challenges
8. **Commit**: "feat: Add NBA API integration for game data"
9. **Push**: Immediate push to GitHub
10. **Update**: Status document and lessons learned

### Session End Checklist
- [ ] All tests passing
- [ ] Code committed and pushed
- [ ] Status document updated
- [ ] Lessons learned captured
- [ ] Next step clearly defined
- [ ] Architecture decisions documented

## ðŸ—ï¸ Project-Specific Patterns

### Python Development
- Use **virtual environments** for dependency isolation
- Follow **PEP 8** style guidelines
- Use **type hints** for better code documentation
- **pytest** for testing framework

### FastAPI Patterns
- **Dependency injection** for database connections
- **Pydantic models** for request/response validation
- **Async/await** for I/O operations
- **Background tasks** for long-running operations

### ML Pipeline
- **Data validation** at ingestion points
- **Model versioning** for reproducibility
- **Feature engineering** documentation
- **Performance monitoring** for model drift

### Sports Data Integration
- **Rate limiting** for API calls
- **Data caching** to reduce API usage
- **Error handling** for API failures
- **Data quality checks** for incoming data

## ðŸš¨ Error Handling & Monitoring

### Lambda Error Handling Pattern
All Lambda functions must implement comprehensive error tracking:

```python
def lambda_handler(event, context):
    """Lambda handler with proper error tracking"""
    try:
        # Main logic here
        result = process_data(event)
        
        return {
            'statusCode': 200,
            'body': json.dumps({'result': result})
        }
        
    except Exception as e:
        print(f"Error: {str(e)}")
        import traceback
        traceback.print_exc()
        
        # Emit CloudWatch metric for monitoring
        try:
            import boto3
            cloudwatch = boto3.client('cloudwatch')
            cloudwatch.put_metric_data(
                Namespace='SportsAnalytics/ServiceName',
                MetricData=[{
                    'MetricName': 'ErrorType',
                    'Value': 1,
                    'Unit': 'Count',
                    'Dimensions': [
                        {'Name': 'Context', 'Value': event.get('key', 'unknown')}
                    ]
                }]
            )
        except:
            pass  # Don't fail on metric emission
        
        return {'statusCode': 500, 'body': json.dumps({'error': str(e)})}
```

### Batch Processing Error Pattern
For functions processing multiple items, track errors without blocking:

```python
def process_batch(items):
    """Process batch with error tracking"""
    error_count = 0
    success_count = 0
    
    for item in items:
        try:
            process_item(item)
            success_count += 1
        except Exception as e:
            error_count += 1
            print(f"Error processing {item}: {e}")
            import traceback
            traceback.print_exc()
    
    # Emit metrics for partial failures
    if error_count > 0:
        emit_metric('ProcessingErrors', error_count)
    
    return success_count
```

### CloudWatch Alarms
All critical Lambda functions have alarms configured:
- **Threshold**: 5 errors per hour
- **Action**: Email notification via SNS
- **Namespace**: `SportsAnalytics/{ServiceName}`
- **Metrics**: Custom error metrics (not just Lambda errors)

### Monitoring Best Practices
1. **Always log full stack traces** for debugging
2. **Emit custom metrics** for business logic errors
3. **Don't block on metric emission** (use try/except)
4. **Track error counts** in batch processing
5. **Distinguish partial vs total failures**
6. **Set appropriate alarm thresholds** per service

---

*Created: 2025-12-28*  
*Updated: 2026-02-22 - Added error handling patterns*  
*Status: Development Workflow Defined*  
*Next: Apply this workflow to current development phase*
